{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Favorfit_image_to_text import clip_image_to_text\n",
    "\n",
    "clip_model = clip_image_to_text.load_interrogator(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/image_to_text/clip\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "tps = glob(\"/media/mlfavorfit/sdb/favorfit_templates/templates/*/*\")\n",
    "tps += glob(\"/media/mlfavorfit/sdb/favorfit_templates/templates_freepik/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "caption_dict = {}\n",
    "for tp in tqdm(tps, total=len(tps)):\n",
    "    img = Image.open(tp)\n",
    "    caption = clip_image_to_text.inference(img, clip_model, mode=\"fast\")\n",
    "    caption_dict[tp] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./caption_dict.json\", mode=\"r\") as f:\n",
    "    json_like = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(json_like.values())\n",
    "texts = [text.split(\", \") for text in texts]\n",
    "main_sent = [text[0] for text in texts]\n",
    "keywords = [text[1:] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_flatten = []\n",
    "for keyword in keywords: keywords_flatten += keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter_keywords = Counter(keywords_flatten)\n",
    "important_keywords = \"natural materials :: high detail, behance. polished, minimalist furniture, soft zen minimalist, trending on textures. com, clean 3 d render, soft geometric 3d shapes, raytraced 3d set design, raytracing shadows, image on the store website, john pawson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "def gen_features(model, text):\n",
    "    tokens = clip.tokenize([text]).to(device)\n",
    "    text_features = model.encode_text(tokens)\n",
    "\n",
    "    return text_features\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    # 정규화\n",
    "    v1_normalized = torch.nn.functional.normalize(v1, p=2, dim=-1)\n",
    "    v2_normalized = torch.nn.functional.normalize(v2, p=2, dim=-1)\n",
    "\n",
    "    # 내적 계산\n",
    "    similarity = torch.matmul(v1_normalized, v2_normalized.transpose(0, 1))\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def dist(caption_embedding, embeddings_tensor):\n",
    "    # 코사인 유사도 계산\n",
    "    similarity = cosine_similarity(caption_embedding, embeddings_tensor)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"ViT-L/14\"\n",
    "model, _ = clip.load(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = [gen_features(model, sentence).to(\"cpu\").squeeze() for sentence in main_sent]\n",
    "embeddings_arr = torch.cat(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"./image/kamil.jpg\")\n",
    "caption = clip_image_to_text.inference(img, clip_model, mode=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    caption_embedding = gen_features(model, caption).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_tensor = dist(caption_embedding, embeddings_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1873, 11528, 17520,  8665,  7951, 16123,  7077, 16894, 14176,  4894,\n",
       "         9968, 18673,  1011,  1008, 14538,  5305,  3193, 12676, 19610,  2706,\n",
       "         9019, 17832, 13041,   946,  3606,  9074,  5067, 17882, 14330,  7036,\n",
       "         5029, 14297, 16087, 12004,  2423, 14430,  5176,  8242,  7146,  7046,\n",
       "        16095,  4844, 14132, 15911,  6843,  3975, 13367,  8541, 13441, 17407])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(dist_tensor, descending=True)[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a cup sitting on top of a table'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_sent[17407]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_rnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
