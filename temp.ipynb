{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_movq_dict = dict(torch.load(\"/home/mlfavorfit/.cache/huggingface/hub/models--kandinsky-community--kandinsky-2-2-controlnet-depth/snapshots/4ecd717e8c9086cf4a16ca28b64894f70a42cd08/movq/diffusion_pytorch_model.bin\"))\n",
    "ct_unet_dict = dict(torch.load(\"/home/mlfavorfit/.cache/huggingface/hub/models--kandinsky-community--kandinsky-2-2-controlnet-depth/snapshots/4ecd717e8c9086cf4a16ca28b64894f70a42cd08/unet/diffusion_pytorch_model.bin\"))\n",
    "\n",
    "un_movq_dict = dict(load_file(\"/home/mlfavorfit/.cache/huggingface/hub/models--kandinsky-community--kandinsky-2-2-decoder-inpaint/snapshots/db790ad5cbcabed886f069ef2710774657621702/movq/diffusion_pytorch_model.safetensors\"))\n",
    "un_unet_dict = dict(load_file(\"/home/mlfavorfit/.cache/huggingface/hub/models--kandinsky-community--kandinsky-2-2-decoder-inpaint/snapshots/db790ad5cbcabed886f069ef2710774657621702/unet/diffusion_pytorch_model.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_movq = sorted([(cur, ct_movq_dict[cur].shape)for cur in ct_movq_dict], key=lambda x:x[0])\n",
    "ct_unet = sorted([(cur, ct_unet_dict[cur].shape)for cur in ct_unet_dict], key=lambda x:x[0])\n",
    "un_movq = sorted([(cur, un_movq_dict[cur].shape)for cur in un_movq_dict], key=lambda x:x[0])\n",
    "un_unet = sorted([(cur, un_unet_dict[cur].shape)for cur in un_unet_dict], key=lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ct, un in zip(ct_movq, un_movq):\n",
    "    if ct != un: print(\"warn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_ct = []\n",
    "common_ct_un = []\n",
    "for name, shape in ct_unet:\n",
    "    if name in un_unet_dict.keys():\n",
    "        common_ct_un.append(name)\n",
    "    else:\n",
    "        only_ct.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_in.weight\n"
     ]
    }
   ],
   "source": [
    "for name in common_ct_un:\n",
    "    if un_unet_dict[name].shape != ct_unet_dict[name].shape: print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 9, 3, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_unet_dict[\"conv_in.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 8, 3, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_unet_dict[\"conv_in.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_embedding.input_hint_block.0.bias torch.Size([16])\n",
      "add_embedding.input_hint_block.0.weight torch.Size([16, 3, 3, 3])\n",
      "add_embedding.input_hint_block.10.bias torch.Size([96])\n",
      "add_embedding.input_hint_block.10.weight torch.Size([96, 96, 3, 3])\n",
      "add_embedding.input_hint_block.12.bias torch.Size([256])\n",
      "add_embedding.input_hint_block.12.weight torch.Size([256, 96, 3, 3])\n",
      "add_embedding.input_hint_block.14.bias torch.Size([4])\n",
      "add_embedding.input_hint_block.14.weight torch.Size([4, 256, 3, 3])\n",
      "add_embedding.input_hint_block.2.bias torch.Size([16])\n",
      "add_embedding.input_hint_block.2.weight torch.Size([16, 16, 3, 3])\n",
      "add_embedding.input_hint_block.4.bias torch.Size([32])\n",
      "add_embedding.input_hint_block.4.weight torch.Size([32, 16, 3, 3])\n",
      "add_embedding.input_hint_block.6.bias torch.Size([32])\n",
      "add_embedding.input_hint_block.6.weight torch.Size([32, 32, 3, 3])\n",
      "add_embedding.input_hint_block.8.bias torch.Size([96])\n",
      "add_embedding.input_hint_block.8.weight torch.Size([96, 32, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for cur in only_ct:\n",
    "    print(cur, ct_unet_dict[cur].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384, 13, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 기존 텐서의 모양과 새로운 모양 정의\n",
    "old_shape = (384, 9, 3, 3)\n",
    "new_shape = (384, 13, 3, 3)\n",
    "\n",
    "# 새로운 텐서 생성 및 기존 텐서 값 복사\n",
    "new_weight = torch.zeros(new_shape)  # 새로운 텐서를 모두 0으로 초기화\n",
    "old_weight = un_unet_dict[\"conv_in.weight\"]\n",
    "new_weight[:, :old_shape[1], :, :] = old_weight\n",
    "\n",
    "# 추가된 부분에 랜덤한 0으로 채우기\n",
    "random_part = torch.zeros((new_shape[0], new_shape[1] - old_shape[1], new_shape[2], new_shape[3]))\n",
    "new_weight[:, old_shape[1]:, :, :] = random_part\n",
    "\n",
    "# 결과 확인\n",
    "print(new_weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_unet_dict[\"conv_in.weight\"] = new_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur in only_ct:\n",
    "    un_unet_dict[cur] = ct_unet_dict[cur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n",
      "torch.Size([384, 13, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(len(un_unet_dict))\n",
    "print(un_unet_dict[\"conv_in.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "save_file(un_unet_dict, \"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-inpainting-controlnet/unet/diffusion_pytorch_model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict = load_file(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-inpainting-controlnet/unet/diffusion_pytorch_model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n",
      "torch.Size([384, 13, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(len(temp_dict))\n",
    "print(temp_dict[\"conv_in.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def downgrade_image(img, bad_size=[64,64]):\n",
    "    img = img.resize(bad_size).resize(img.size)\n",
    "    return img\n",
    "\n",
    "from utils import make_canny_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "fns = glob(\"/media/mlfavorfit/sdb/contolnet_dataset/control_net_train_bad/images/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197967/197967 [46:24<00:00, 71.10it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_dir = \"/media/mlfavorfit/sdb/contolnet_dataset/control_net_train_bad/conditioning_images_canny\"\n",
    "for fn in tqdm(fns, total=len(fns)):\n",
    "    img = Image.open(fn)\n",
    "    bad_img = make_canny_condition(img)\n",
    "    bad_img.save(os.path.join(save_dir, os.path.basename(fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "temp_key = torch.load(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder/controlnet/ckpt_63000_0313/diffusion_pytorch_model.bin\", map_location=\"cpu\").keys()\n",
    "temp_key = list(temp_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "key_list = list(load_file(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder/unet/diffusion_pytorch_model.safetensors\").keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([cur for cur in key_list if not cur.startswith(\"up\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid_block.resnets.0.conv1.bias\n",
      "mid_block.resnets.0.conv1.weight\n",
      "mid_block.resnets.0.conv2.bias\n",
      "mid_block.resnets.0.conv2.weight\n",
      "mid_block.resnets.0.norm1.bias\n",
      "mid_block.resnets.0.norm1.weight\n",
      "mid_block.resnets.0.norm2.bias\n",
      "mid_block.resnets.0.norm2.weight\n",
      "mid_block.resnets.0.time_emb_proj.bias\n",
      "mid_block.resnets.0.time_emb_proj.weight\n",
      "mid_block.resnets.1.conv1.bias\n",
      "mid_block.resnets.1.conv1.weight\n",
      "mid_block.resnets.1.conv2.bias\n",
      "mid_block.resnets.1.conv2.weight\n",
      "mid_block.resnets.1.norm1.bias\n",
      "mid_block.resnets.1.norm1.weight\n",
      "mid_block.resnets.1.norm2.bias\n",
      "mid_block.resnets.1.norm2.weight\n",
      "mid_block.resnets.1.time_emb_proj.bias\n",
      "mid_block.resnets.1.time_emb_proj.weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 0\n",
    "for key in key_list:\n",
    "    if key in temp_key:\n",
    "        temp+=1\n",
    "        if \"mid\" in key:\n",
    "            print(key)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controlnet_cond_embedding.conv_in.weight\n",
      "controlnet_cond_embedding.conv_in.bias\n",
      "controlnet_cond_embedding.blocks.0.weight\n",
      "controlnet_cond_embedding.blocks.0.bias\n",
      "controlnet_cond_embedding.blocks.1.weight\n",
      "controlnet_cond_embedding.blocks.1.bias\n",
      "controlnet_cond_embedding.blocks.2.weight\n",
      "controlnet_cond_embedding.blocks.2.bias\n",
      "controlnet_cond_embedding.blocks.3.weight\n",
      "controlnet_cond_embedding.blocks.3.bias\n",
      "controlnet_cond_embedding.blocks.4.weight\n",
      "controlnet_cond_embedding.blocks.4.bias\n",
      "controlnet_cond_embedding.blocks.5.weight\n",
      "controlnet_cond_embedding.blocks.5.bias\n",
      "controlnet_cond_embedding.conv_out.weight\n",
      "controlnet_cond_embedding.conv_out.bias\n",
      "controlnet_down_blocks.0.weight\n",
      "controlnet_down_blocks.0.bias\n",
      "controlnet_down_blocks.1.weight\n",
      "controlnet_down_blocks.1.bias\n",
      "controlnet_down_blocks.2.weight\n",
      "controlnet_down_blocks.2.bias\n",
      "controlnet_down_blocks.3.weight\n",
      "controlnet_down_blocks.3.bias\n",
      "controlnet_down_blocks.4.weight\n",
      "controlnet_down_blocks.4.bias\n",
      "controlnet_down_blocks.5.weight\n",
      "controlnet_down_blocks.5.bias\n",
      "controlnet_down_blocks.6.weight\n",
      "controlnet_down_blocks.6.bias\n",
      "controlnet_down_blocks.7.weight\n",
      "controlnet_down_blocks.7.bias\n",
      "controlnet_down_blocks.8.weight\n",
      "controlnet_down_blocks.8.bias\n",
      "controlnet_down_blocks.9.weight\n",
      "controlnet_down_blocks.9.bias\n",
      "controlnet_down_blocks.10.weight\n",
      "controlnet_down_blocks.10.bias\n",
      "controlnet_down_blocks.11.weight\n",
      "controlnet_down_blocks.11.bias\n",
      "controlnet_down_blocks.12.weight\n",
      "controlnet_down_blocks.12.bias\n",
      "controlnet_down_blocks.13.weight\n",
      "controlnet_down_blocks.13.bias\n",
      "controlnet_down_blocks.14.weight\n",
      "controlnet_down_blocks.14.bias\n",
      "controlnet_down_blocks.15.weight\n",
      "controlnet_down_blocks.15.bias\n",
      "controlnet_mid_block.weight\n",
      "controlnet_mid_block.bias\n",
      "mid_block.attentions.0.norm.weight\n",
      "mid_block.attentions.0.norm.bias\n",
      "mid_block.attentions.0.proj_in.weight\n",
      "mid_block.attentions.0.proj_in.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "mid_block.attentions.0.proj_out.weight\n",
      "mid_block.attentions.0.proj_out.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 0\n",
    "for key in temp_key:\n",
    "    if key not in key_list:\n",
    "        print(key)\n",
    "        temp+=1\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 0\n",
    "for key in key_list:\n",
    "    if key not in temp_key:\n",
    "        temp+=1\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([cur for cur in key_list if not cur.startswith(\"up\")]) + 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlfavorfit/anaconda3/envs/diffusion_rnd/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "from PIL.ImageOps import expand\n",
    "\n",
    "from diffusers.utils import make_image_grid\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "from diffusers import KandinskyV22PriorPipeline\n",
    "from diffusers.pipelines.kandinsky2_2.pipline_kandinsky2_2_controlnet_split import KandinskyV22ControlnetPipeline\n",
    "\n",
    "from diffusers.models.controlnet_kandinsky import ControlNetModel as ControlNetModelKandinsky\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'center_input_sample': False} were passed to ControlNetModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot load <class 'diffusers.models.controlnet_kandinsky.ControlNetModel'> from /home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder because the following keys are missing: \n down_blocks.1.resnets.2.norm1.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm3.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight, controlnet_down_blocks.15.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight, down_blocks.0.resnets.2.norm1.bias, down_blocks.2.attentions.2.transformer_blocks.0.norm2.weight, down_blocks.1.resnets.2.time_emb_proj.weight, down_blocks.2.attentions.2.norm.weight, controlnet_down_blocks.15.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.0.attentions.2.transformer_blocks.0.norm3.bias, down_blocks.3.resnets.2.conv1.bias, down_blocks.0.resnets.2.norm1.weight, down_blocks.1.resnets.2.conv2.bias, down_blocks.0.resnets.2.norm2.bias, down_blocks.2.resnets.2.norm1.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm2.bias, down_blocks.0.resnets.2.conv2.weight, down_blocks.3.resnets.2.conv2.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, down_blocks.1.attentions.2.transformer_blocks.0.norm2.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight, controlnet_down_blocks.12.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight, down_blocks.0.resnets.2.time_emb_proj.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm1.bias, down_blocks.1.attentions.2.proj_out.weight, down_blocks.1.attentions.2.proj_in.bias, down_blocks.1.resnets.2.conv2.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight, down_blocks.2.attentions.2.proj_out.bias, down_blocks.0.attentions.2.proj_out.weight, down_blocks.0.resnets.2.conv1.weight, down_blocks.3.resnets.2.conv2.weight, down_blocks.0.attentions.2.proj_out.bias, down_blocks.0.attentions.2.transformer_blocks.0.norm3.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight, down_blocks.0.attentions.2.transformer_blocks.0.norm1.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, controlnet_proj_blocks.15.bias, down_blocks.3.resnets.2.norm2.weight, down_blocks.2.attentions.2.proj_in.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm1.weight, controlnet_proj_blocks.12.weight, down_blocks.1.resnets.2.time_emb_proj.bias, down_blocks.2.resnets.2.time_emb_proj.bias, down_blocks.2.attentions.2.proj_in.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, controlnet_proj_blocks.14.bias, down_blocks.0.attentions.2.norm.bias, controlnet_proj_blocks.13.bias, down_blocks.2.resnets.2.time_emb_proj.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm3.bias, controlnet_proj_blocks.12.bias, down_blocks.1.attentions.2.norm.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias, down_blocks.0.resnets.2.conv1.bias, down_blocks.2.resnets.2.conv2.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.0.attentions.2.proj_in.weight, down_blocks.2.attentions.2.proj_out.weight, controlnet_proj_blocks.13.weight, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias, down_blocks.1.resnets.2.norm1.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight, down_blocks.2.resnets.2.norm1.bias, down_blocks.1.attentions.2.transformer_blocks.0.norm2.bias, down_blocks.1.resnets.2.conv1.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight, down_blocks.0.attentions.2.transformer_blocks.0.norm2.weight, down_blocks.1.attentions.2.norm.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, down_blocks.2.resnets.2.conv1.bias, down_blocks.2.resnets.2.norm2.weight, down_blocks.1.attentions.2.proj_in.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm3.weight, down_blocks.1.resnets.2.norm2.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight, down_blocks.2.resnets.2.conv2.bias, down_blocks.3.resnets.2.norm1.bias, down_blocks.3.resnets.2.conv1.weight, down_blocks.0.resnets.2.conv2.bias, down_blocks.0.attentions.2.transformer_blocks.0.norm2.bias, down_blocks.2.attentions.2.transformer_blocks.0.norm1.bias, down_blocks.0.resnets.2.time_emb_proj.bias, down_blocks.2.resnets.2.conv1.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias, controlnet_down_blocks.12.weight, down_blocks.1.resnets.2.norm2.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight, down_blocks.1.resnets.2.conv1.weight, down_blocks.0.attentions.2.norm.weight, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm3.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight, down_blocks.3.resnets.2.norm1.weight, down_blocks.2.resnets.2.norm2.bias, down_blocks.3.resnets.2.norm2.bias, down_blocks.2.attentions.2.norm.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm1.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, down_blocks.3.resnets.2.time_emb_proj.weight, down_blocks.0.resnets.2.norm2.weight, controlnet_proj_blocks.15.weight, controlnet_down_blocks.14.bias, down_blocks.0.attentions.2.proj_in.bias, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias, controlnet_down_blocks.13.bias, controlnet_down_blocks.14.weight, down_blocks.1.attentions.2.proj_out.bias, down_blocks.3.resnets.2.time_emb_proj.bias, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight, controlnet_proj_blocks.14.weight, down_blocks.0.attentions.2.transformer_blocks.0.norm1.bias, controlnet_down_blocks.13.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight. \n Please make sure to pass `low_cpu_mem_usage=False` and `device_map=None` if you want to randomly initialize those weights or else make sure your checkpoint file is correct.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m decoder_model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m prior_model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-prior\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m controlnet_kandinsky \u001b[39m=\u001b[39m ControlNetModelKandinsky\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m     decoder_model_path, subfolder\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcontrolnet/tile_from_sd15\u001b[39;49m\u001b[39m\"\u001b[39;49m, torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16,\n\u001b[1;32m      6\u001b[0m )\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion_rnd/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/lib/favorfit/kjg/diffusers_research/diffusers_research/diffusers/models/modeling_utils.py:662\u001b[0m, in \u001b[0;36mModelMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m missing_keys \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(model\u001b[39m.\u001b[39mstate_dict()\u001b[39m.\u001b[39mkeys()) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(state_dict\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m    661\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_keys) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 662\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    663\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot load \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m because the following keys are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    664\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m missing: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(missing_keys)\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Please make sure to pass\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    665\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `low_cpu_mem_usage=False` and `device_map=None` if you want to randomly initialize\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m those weights or else make sure your checkpoint file is correct.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    667\u001b[0m     )\n\u001b[1;32m    669\u001b[0m unexpected_keys \u001b[39m=\u001b[39m load_model_dict_into_meta(\n\u001b[1;32m    670\u001b[0m     model,\n\u001b[1;32m    671\u001b[0m     state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     model_name_or_path\u001b[39m=\u001b[39mpretrained_model_name_or_path,\n\u001b[1;32m    675\u001b[0m )\n\u001b[1;32m    677\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_keys_to_ignore_on_load_unexpected \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load <class 'diffusers.models.controlnet_kandinsky.ControlNetModel'> from /home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder because the following keys are missing: \n down_blocks.1.resnets.2.norm1.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm3.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight, controlnet_down_blocks.15.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight, down_blocks.0.resnets.2.norm1.bias, down_blocks.2.attentions.2.transformer_blocks.0.norm2.weight, down_blocks.1.resnets.2.time_emb_proj.weight, down_blocks.2.attentions.2.norm.weight, controlnet_down_blocks.15.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.0.attentions.2.transformer_blocks.0.norm3.bias, down_blocks.3.resnets.2.conv1.bias, down_blocks.0.resnets.2.norm1.weight, down_blocks.1.resnets.2.conv2.bias, down_blocks.0.resnets.2.norm2.bias, down_blocks.2.resnets.2.norm1.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm2.bias, down_blocks.0.resnets.2.conv2.weight, down_blocks.3.resnets.2.conv2.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, down_blocks.1.attentions.2.transformer_blocks.0.norm2.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight, controlnet_down_blocks.12.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight, down_blocks.0.resnets.2.time_emb_proj.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm1.bias, down_blocks.1.attentions.2.proj_out.weight, down_blocks.1.attentions.2.proj_in.bias, down_blocks.1.resnets.2.conv2.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight, down_blocks.2.attentions.2.proj_out.bias, down_blocks.0.attentions.2.proj_out.weight, down_blocks.0.resnets.2.conv1.weight, down_blocks.3.resnets.2.conv2.weight, down_blocks.0.attentions.2.proj_out.bias, down_blocks.0.attentions.2.transformer_blocks.0.norm3.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight, down_blocks.0.attentions.2.transformer_blocks.0.norm1.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, controlnet_proj_blocks.15.bias, down_blocks.3.resnets.2.norm2.weight, down_blocks.2.attentions.2.proj_in.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm1.weight, controlnet_proj_blocks.12.weight, down_blocks.1.resnets.2.time_emb_proj.bias, down_blocks.2.resnets.2.time_emb_proj.bias, down_blocks.2.attentions.2.proj_in.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, controlnet_proj_blocks.14.bias, down_blocks.0.attentions.2.norm.bias, controlnet_proj_blocks.13.bias, down_blocks.2.resnets.2.time_emb_proj.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm3.bias, controlnet_proj_blocks.12.bias, down_blocks.1.attentions.2.norm.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias, down_blocks.0.resnets.2.conv1.bias, down_blocks.2.resnets.2.conv2.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.0.attentions.2.proj_in.weight, down_blocks.2.attentions.2.proj_out.weight, controlnet_proj_blocks.13.weight, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias, down_blocks.1.resnets.2.norm1.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight, down_blocks.2.resnets.2.norm1.bias, down_blocks.1.attentions.2.transformer_blocks.0.norm2.bias, down_blocks.1.resnets.2.conv1.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight, down_blocks.0.attentions.2.transformer_blocks.0.norm2.weight, down_blocks.1.attentions.2.norm.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, down_blocks.2.resnets.2.conv1.bias, down_blocks.2.resnets.2.norm2.weight, down_blocks.1.attentions.2.proj_in.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm3.weight, down_blocks.1.resnets.2.norm2.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight, down_blocks.2.resnets.2.conv2.bias, down_blocks.3.resnets.2.norm1.bias, down_blocks.3.resnets.2.conv1.weight, down_blocks.0.resnets.2.conv2.bias, down_blocks.0.attentions.2.transformer_blocks.0.norm2.bias, down_blocks.2.attentions.2.transformer_blocks.0.norm1.bias, down_blocks.0.resnets.2.time_emb_proj.bias, down_blocks.2.resnets.2.conv1.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias, controlnet_down_blocks.12.weight, down_blocks.1.resnets.2.norm2.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight, down_blocks.1.resnets.2.conv1.weight, down_blocks.0.attentions.2.norm.weight, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm3.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight, down_blocks.3.resnets.2.norm1.weight, down_blocks.2.resnets.2.norm2.bias, down_blocks.3.resnets.2.norm2.bias, down_blocks.2.attentions.2.norm.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm1.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, down_blocks.3.resnets.2.time_emb_proj.weight, down_blocks.0.resnets.2.norm2.weight, controlnet_proj_blocks.15.weight, controlnet_down_blocks.14.bias, down_blocks.0.attentions.2.proj_in.bias, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias, controlnet_down_blocks.13.bias, controlnet_down_blocks.14.weight, down_blocks.1.attentions.2.proj_out.bias, down_blocks.3.resnets.2.time_emb_proj.bias, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight, controlnet_proj_blocks.14.weight, down_blocks.0.attentions.2.transformer_blocks.0.norm1.bias, controlnet_down_blocks.13.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight. \n Please make sure to pass `low_cpu_mem_usage=False` and `device_map=None` if you want to randomly initialize those weights or else make sure your checkpoint file is correct."
     ]
    }
   ],
   "source": [
    "decoder_model_path = \"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder\"\n",
    "prior_model_path = \"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-prior\"\n",
    "\n",
    "controlnet_kandinsky = ControlNetModelKandinsky.from_pretrained(\n",
    "    decoder_model_path, subfolder=\"controlnet/tile_from_sd15\", torch_dtype=torch.float16,\n",
    ").eval()\n",
    "\n",
    "# prior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\n",
    "#     prior_model_path,\n",
    "#     use_safetensors=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "\n",
    "# pipeline = KandinskyV22ControlnetPipeline.from_pretrained(\n",
    "#     decoder_model_path,\n",
    "#     controlnet=controlnet_kandinsky,\n",
    "#     torch_dtype=torch.float16,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlfavorfit/lib/favorfit/kjg/diffusers_research/diffusers_research/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'diffusers.models.controlnet_kandinsky.ControlNetModel'>.load_config(...) followed by <class 'diffusers.models.controlnet_kandinsky.ControlNetModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "The config attributes {'center_input_sample': False} were passed to ControlNetModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "wei = ControlNetModelKandinsky.from_config(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder/controlnet/tile_from_sd15/config.json\").state_dict()\n",
    "w = torch.load(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder/controlnet/tile_from_sd15/diffusion_pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down_blocks.0.attentions.2.norm.weight\n",
      "down_blocks.0.attentions.2.norm.bias\n",
      "down_blocks.0.attentions.2.proj_in.weight\n",
      "down_blocks.0.attentions.2.proj_in.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.norm1.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.norm1.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.norm2.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.norm2.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.norm3.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.norm3.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias\n",
      "down_blocks.0.attentions.2.proj_out.weight\n",
      "down_blocks.0.attentions.2.proj_out.bias\n",
      "down_blocks.0.resnets.2.norm1.weight\n",
      "down_blocks.0.resnets.2.norm1.bias\n",
      "down_blocks.0.resnets.2.conv1.weight\n",
      "down_blocks.0.resnets.2.conv1.bias\n",
      "down_blocks.0.resnets.2.time_emb_proj.weight\n",
      "down_blocks.0.resnets.2.time_emb_proj.bias\n",
      "down_blocks.0.resnets.2.norm2.weight\n",
      "down_blocks.0.resnets.2.norm2.bias\n",
      "down_blocks.0.resnets.2.conv2.weight\n",
      "down_blocks.0.resnets.2.conv2.bias\n",
      "down_blocks.1.attentions.2.norm.weight\n",
      "down_blocks.1.attentions.2.norm.bias\n",
      "down_blocks.1.attentions.2.proj_in.weight\n",
      "down_blocks.1.attentions.2.proj_in.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.norm1.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.norm1.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.norm2.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.norm2.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.norm3.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.norm3.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias\n",
      "down_blocks.1.attentions.2.proj_out.weight\n",
      "down_blocks.1.attentions.2.proj_out.bias\n",
      "down_blocks.1.resnets.2.norm1.weight\n",
      "down_blocks.1.resnets.2.norm1.bias\n",
      "down_blocks.1.resnets.2.conv1.weight\n",
      "down_blocks.1.resnets.2.conv1.bias\n",
      "down_blocks.1.resnets.2.time_emb_proj.weight\n",
      "down_blocks.1.resnets.2.time_emb_proj.bias\n",
      "down_blocks.1.resnets.2.norm2.weight\n",
      "down_blocks.1.resnets.2.norm2.bias\n",
      "down_blocks.1.resnets.2.conv2.weight\n",
      "down_blocks.1.resnets.2.conv2.bias\n",
      "down_blocks.2.attentions.2.norm.weight\n",
      "down_blocks.2.attentions.2.norm.bias\n",
      "down_blocks.2.attentions.2.proj_in.weight\n",
      "down_blocks.2.attentions.2.proj_in.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.norm1.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.norm1.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.norm2.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.norm2.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.norm3.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.norm3.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias\n",
      "down_blocks.2.attentions.2.proj_out.weight\n",
      "down_blocks.2.attentions.2.proj_out.bias\n",
      "down_blocks.2.resnets.2.norm1.weight\n",
      "down_blocks.2.resnets.2.norm1.bias\n",
      "down_blocks.2.resnets.2.conv1.weight\n",
      "down_blocks.2.resnets.2.conv1.bias\n",
      "down_blocks.2.resnets.2.time_emb_proj.weight\n",
      "down_blocks.2.resnets.2.time_emb_proj.bias\n",
      "down_blocks.2.resnets.2.norm2.weight\n",
      "down_blocks.2.resnets.2.norm2.bias\n",
      "down_blocks.2.resnets.2.conv2.weight\n",
      "down_blocks.2.resnets.2.conv2.bias\n",
      "down_blocks.3.resnets.2.norm1.weight\n",
      "down_blocks.3.resnets.2.norm1.bias\n",
      "down_blocks.3.resnets.2.conv1.weight\n",
      "down_blocks.3.resnets.2.conv1.bias\n",
      "down_blocks.3.resnets.2.time_emb_proj.weight\n",
      "down_blocks.3.resnets.2.time_emb_proj.bias\n",
      "down_blocks.3.resnets.2.norm2.weight\n",
      "down_blocks.3.resnets.2.norm2.bias\n",
      "down_blocks.3.resnets.2.conv2.weight\n",
      "down_blocks.3.resnets.2.conv2.bias\n",
      "controlnet_down_blocks.0.weight\n",
      "controlnet_down_blocks.0.bias\n",
      "controlnet_down_blocks.1.weight\n",
      "controlnet_down_blocks.1.bias\n",
      "controlnet_down_blocks.2.weight\n",
      "controlnet_down_blocks.2.bias\n",
      "controlnet_down_blocks.3.weight\n",
      "controlnet_down_blocks.3.bias\n",
      "controlnet_down_blocks.4.weight\n",
      "controlnet_down_blocks.4.bias\n",
      "controlnet_down_blocks.5.weight\n",
      "controlnet_down_blocks.5.bias\n",
      "controlnet_down_blocks.6.weight\n",
      "controlnet_down_blocks.6.bias\n",
      "controlnet_down_blocks.7.weight\n",
      "controlnet_down_blocks.7.bias\n",
      "controlnet_down_blocks.8.weight\n",
      "controlnet_down_blocks.8.bias\n",
      "controlnet_down_blocks.9.weight\n",
      "controlnet_down_blocks.9.bias\n",
      "controlnet_down_blocks.10.weight\n",
      "controlnet_down_blocks.10.bias\n",
      "controlnet_down_blocks.11.weight\n",
      "controlnet_down_blocks.11.bias\n",
      "controlnet_down_blocks.12.weight\n",
      "controlnet_down_blocks.12.bias\n",
      "controlnet_down_blocks.13.weight\n",
      "controlnet_down_blocks.13.bias\n",
      "controlnet_down_blocks.14.weight\n",
      "controlnet_down_blocks.14.bias\n",
      "controlnet_down_blocks.15.weight\n",
      "controlnet_down_blocks.15.bias\n",
      "controlnet_proj_blocks.0.weight\n",
      "controlnet_proj_blocks.0.bias\n",
      "controlnet_proj_blocks.1.weight\n",
      "controlnet_proj_blocks.1.bias\n",
      "controlnet_proj_blocks.2.weight\n",
      "controlnet_proj_blocks.2.bias\n",
      "controlnet_proj_blocks.3.weight\n",
      "controlnet_proj_blocks.3.bias\n",
      "controlnet_proj_blocks.4.weight\n",
      "controlnet_proj_blocks.4.bias\n",
      "controlnet_proj_blocks.5.weight\n",
      "controlnet_proj_blocks.5.bias\n",
      "controlnet_proj_blocks.6.weight\n",
      "controlnet_proj_blocks.6.bias\n",
      "controlnet_proj_blocks.7.weight\n",
      "controlnet_proj_blocks.7.bias\n",
      "controlnet_proj_blocks.8.weight\n",
      "controlnet_proj_blocks.8.bias\n",
      "controlnet_proj_blocks.9.weight\n",
      "controlnet_proj_blocks.9.bias\n",
      "controlnet_proj_blocks.10.weight\n",
      "controlnet_proj_blocks.10.bias\n",
      "controlnet_proj_blocks.11.weight\n",
      "controlnet_proj_blocks.11.bias\n",
      "controlnet_proj_blocks.12.weight\n",
      "controlnet_proj_blocks.12.bias\n",
      "controlnet_proj_blocks.13.weight\n",
      "controlnet_proj_blocks.13.bias\n",
      "controlnet_proj_blocks.14.weight\n",
      "controlnet_proj_blocks.14.bias\n",
      "controlnet_proj_blocks.15.weight\n",
      "controlnet_proj_blocks.15.bias\n",
      "controlnet_mid_proj_block.weight\n",
      "controlnet_mid_proj_block.bias\n",
      "controlnet_mid_block.weight\n",
      "controlnet_mid_block.bias\n"
     ]
    }
   ],
   "source": [
    "mu = 0.0\n",
    "sigma = 0.2\n",
    "\n",
    "for cur in wei.keys():\n",
    "    if cur in w.keys():\n",
    "        if wei[cur].shape != w[cur].shape: \n",
    "            print(cur)\n",
    "            if cur.startswith(\"controlnet_down_blocks\") or cur.startswith(\"controlnet_mid_block\"):\n",
    "                wei[cur] = torch.zeros_like(wei[cur])\n",
    "            else:\n",
    "                wei[cur] = sigma * torch.randn_like(wei[cur], dtype=torch.float16)\n",
    "        else:\n",
    "            wei[cur] = w[cur]\n",
    "    else:\n",
    "        print(cur)\n",
    "        wei[cur] = sigma * torch.randn_like(wei[cur], dtype=torch.float16)\n",
    "        if cur.startswith(\"controlnet_down_blocks\") or cur.startswith(\"controlnet_mid_block\"):\n",
    "            wei[cur] = torch.zeros_like(wei[cur])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3904, dtype=torch.float16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(wei[\"down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[\"controlnet_mid_block.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1699,  0.2378, -0.0591],\n",
       "          [ 0.2622, -0.4448,  0.3770],\n",
       "          [-0.3022,  0.3354, -0.2798]],\n",
       "\n",
       "         [[-0.2104,  0.2266, -0.0986],\n",
       "          [-0.1406,  0.0220,  0.3018],\n",
       "          [-0.4321, -0.0869, -0.1113]],\n",
       "\n",
       "         [[-0.3682,  0.0581, -0.2690],\n",
       "          [ 0.3193,  0.0918,  0.4707],\n",
       "          [-0.1680,  0.1436,  0.4478]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0620,  0.2100, -0.1523],\n",
       "          [ 0.3569, -0.0557,  0.4468],\n",
       "          [ 0.4136,  0.0400, -0.3149]],\n",
       "\n",
       "         [[-0.3472,  0.3955,  0.4028],\n",
       "          [ 0.3403, -0.1479, -0.3335],\n",
       "          [ 0.2852, -0.3320, -0.2822]],\n",
       "\n",
       "         [[ 0.2588,  0.4580, -0.0786],\n",
       "          [ 0.3955,  0.2012,  0.0576],\n",
       "          [-0.2373,  0.1382, -0.3965]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3003,  0.4888,  0.0361],\n",
       "          [-0.1621, -0.4673,  0.4399],\n",
       "          [ 0.4678, -0.4048,  0.0020]],\n",
       "\n",
       "         [[ 0.1050,  0.3672,  0.3799],\n",
       "          [-0.2139, -0.0591, -0.1265],\n",
       "          [-0.1812,  0.0527, -0.1758]],\n",
       "\n",
       "         [[ 0.2573,  0.3120, -0.3379],\n",
       "          [-0.1519,  0.1377, -0.1436],\n",
       "          [-0.4248,  0.0327, -0.4009]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2827,  0.4404,  0.4468],\n",
       "          [-0.2061, -0.1758,  0.3643],\n",
       "          [ 0.4580, -0.0864, -0.2070]],\n",
       "\n",
       "         [[ 0.0703,  0.1387,  0.3276],\n",
       "          [-0.2139, -0.0659,  0.0752],\n",
       "          [ 0.2207, -0.3271, -0.0259]],\n",
       "\n",
       "         [[ 0.3867, -0.1357,  0.1353],\n",
       "          [ 0.0342, -0.4736,  0.4238],\n",
       "          [ 0.2041, -0.3604,  0.4160]]],\n",
       "\n",
       "\n",
       "        [[[-0.3857, -0.1738,  0.4814],\n",
       "          [-0.3989,  0.1528,  0.0581],\n",
       "          [-0.2983,  0.1748, -0.4790]],\n",
       "\n",
       "         [[ 0.4805,  0.1802, -0.4395],\n",
       "          [ 0.0518,  0.4824, -0.4570],\n",
       "          [ 0.2529,  0.0591, -0.2954]],\n",
       "\n",
       "         [[-0.1831,  0.3647, -0.3198],\n",
       "          [-0.4463, -0.2959,  0.4863],\n",
       "          [ 0.2090,  0.1782,  0.2686]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.3828,  0.0923,  0.0566],\n",
       "          [ 0.0850,  0.4521, -0.4219],\n",
       "          [-0.0513, -0.1665,  0.2100]],\n",
       "\n",
       "         [[-0.2031,  0.2188, -0.0044],\n",
       "          [ 0.4722, -0.4453, -0.1270],\n",
       "          [ 0.0405, -0.1831, -0.0337]],\n",
       "\n",
       "         [[-0.0820, -0.4092,  0.3086],\n",
       "          [ 0.1953, -0.2510,  0.0986],\n",
       "          [ 0.3516, -0.1846,  0.4302]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.3979,  0.0098,  0.0952],\n",
       "          [-0.3750,  0.2910, -0.0845],\n",
       "          [ 0.1094, -0.3887,  0.2158]],\n",
       "\n",
       "         [[-0.3047,  0.2310, -0.4590],\n",
       "          [-0.4004, -0.4844, -0.0972],\n",
       "          [ 0.2803, -0.3467,  0.3037]],\n",
       "\n",
       "         [[ 0.1821, -0.1206,  0.0532],\n",
       "          [-0.2661, -0.1406,  0.4038],\n",
       "          [-0.3071,  0.1919, -0.2637]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0264, -0.2188,  0.1030],\n",
       "          [ 0.3599, -0.3022,  0.1978],\n",
       "          [-0.1768, -0.1387,  0.2969]],\n",
       "\n",
       "         [[-0.1890,  0.0679,  0.3208],\n",
       "          [-0.3433,  0.3438,  0.0430],\n",
       "          [-0.4170, -0.1226, -0.3613]],\n",
       "\n",
       "         [[ 0.2837, -0.3184,  0.0737],\n",
       "          [ 0.1323,  0.0347,  0.1675],\n",
       "          [-0.4331,  0.1294, -0.2407]]],\n",
       "\n",
       "\n",
       "        [[[-0.3315, -0.3989,  0.0698],\n",
       "          [-0.3960, -0.0801,  0.1113],\n",
       "          [ 0.4204,  0.1846,  0.2026]],\n",
       "\n",
       "         [[ 0.4858,  0.0078,  0.3442],\n",
       "          [ 0.2192,  0.1636,  0.2964],\n",
       "          [-0.1914,  0.2549,  0.0562]],\n",
       "\n",
       "         [[ 0.4556, -0.4912, -0.3975],\n",
       "          [-0.1758,  0.1240,  0.4834],\n",
       "          [ 0.3413, -0.3691, -0.4258]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1948, -0.1934, -0.0122],\n",
       "          [-0.1064,  0.1479, -0.4238],\n",
       "          [ 0.1851, -0.4692,  0.0220]],\n",
       "\n",
       "         [[-0.4102, -0.3809,  0.2427],\n",
       "          [ 0.4854, -0.3057, -0.1519],\n",
       "          [ 0.2812,  0.3188,  0.0137]],\n",
       "\n",
       "         [[-0.3804,  0.4351, -0.0776],\n",
       "          [ 0.0693, -0.3618,  0.3608],\n",
       "          [ 0.2378, -0.0742,  0.1616]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2100,  0.4155, -0.1494],\n",
       "          [-0.0186, -0.3848,  0.2666],\n",
       "          [-0.2837,  0.0742,  0.0024]],\n",
       "\n",
       "         [[ 0.4565, -0.2041, -0.1196],\n",
       "          [ 0.2588,  0.3696,  0.0381],\n",
       "          [-0.2173, -0.4976, -0.2568]],\n",
       "\n",
       "         [[ 0.2510,  0.3120, -0.0498],\n",
       "          [ 0.4150,  0.0859, -0.3081],\n",
       "          [ 0.1060, -0.4951, -0.4443]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.4082,  0.1929,  0.1143],\n",
       "          [ 0.4443,  0.1772, -0.2427],\n",
       "          [-0.4277, -0.1323, -0.1025]],\n",
       "\n",
       "         [[ 0.1851, -0.1587,  0.4912],\n",
       "          [-0.3125, -0.4185, -0.1865],\n",
       "          [-0.4316,  0.1357,  0.1851]],\n",
       "\n",
       "         [[ 0.3374,  0.3247,  0.2100],\n",
       "          [-0.3887, -0.0142,  0.3311],\n",
       "          [ 0.2725, -0.3496, -0.3911]]]], dtype=torch.float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[\"controlnet_proj_blocks.12.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(wei, \"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder/controlnet/tile_from_sd15/diffusion_pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./temp.jsonl\", mode=\"r\") as f:\n",
    "    temp = [cur[:-1] for cur in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down_blocks.0.attentions.2.norm.weight',\n",
       " 'down_blocks.0.attentions.2.norm.bias',\n",
       " 'down_blocks.0.attentions.2.proj_in.weight',\n",
       " 'down_blocks.0.attentions.2.proj_in.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.norm1.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.norm1.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.norm2.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.norm2.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.norm3.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.norm3.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias',\n",
       " 'down_blocks.0.attentions.2.proj_out.weight',\n",
       " 'down_blocks.0.attentions.2.proj_out.bias',\n",
       " 'down_blocks.0.resnets.2.norm1.weight',\n",
       " 'down_blocks.0.resnets.2.norm1.bias',\n",
       " 'down_blocks.0.resnets.2.conv1.weight',\n",
       " 'down_blocks.0.resnets.2.conv1.bias',\n",
       " 'down_blocks.0.resnets.2.time_emb_proj.weight',\n",
       " 'down_blocks.0.resnets.2.time_emb_proj.bias',\n",
       " 'down_blocks.0.resnets.2.norm2.weight',\n",
       " 'down_blocks.0.resnets.2.norm2.bias',\n",
       " 'down_blocks.0.resnets.2.conv2.weight',\n",
       " 'down_blocks.0.resnets.2.conv2.bias',\n",
       " 'down_blocks.1.attentions.2.norm.weight',\n",
       " 'down_blocks.1.attentions.2.norm.bias',\n",
       " 'down_blocks.1.attentions.2.proj_in.weight',\n",
       " 'down_blocks.1.attentions.2.proj_in.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.norm1.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.norm1.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.norm2.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.norm2.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.norm3.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.norm3.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias',\n",
       " 'down_blocks.1.attentions.2.proj_out.weight',\n",
       " 'down_blocks.1.attentions.2.proj_out.bias',\n",
       " 'down_blocks.1.resnets.2.norm1.weight',\n",
       " 'down_blocks.1.resnets.2.norm1.bias',\n",
       " 'down_blocks.1.resnets.2.conv1.weight',\n",
       " 'down_blocks.1.resnets.2.conv1.bias',\n",
       " 'down_blocks.1.resnets.2.time_emb_proj.weight',\n",
       " 'down_blocks.1.resnets.2.time_emb_proj.bias',\n",
       " 'down_blocks.1.resnets.2.norm2.weight',\n",
       " 'down_blocks.1.resnets.2.norm2.bias',\n",
       " 'down_blocks.1.resnets.2.conv2.weight',\n",
       " 'down_blocks.1.resnets.2.conv2.bias',\n",
       " 'down_blocks.2.attentions.2.norm.weight',\n",
       " 'down_blocks.2.attentions.2.norm.bias',\n",
       " 'down_blocks.2.attentions.2.proj_in.weight',\n",
       " 'down_blocks.2.attentions.2.proj_in.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.norm1.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.norm1.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.norm2.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.norm2.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.norm3.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.norm3.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias',\n",
       " 'down_blocks.2.attentions.2.proj_out.weight',\n",
       " 'down_blocks.2.attentions.2.proj_out.bias',\n",
       " 'down_blocks.2.resnets.2.norm1.weight',\n",
       " 'down_blocks.2.resnets.2.norm1.bias',\n",
       " 'down_blocks.2.resnets.2.conv1.weight',\n",
       " 'down_blocks.2.resnets.2.conv1.bias',\n",
       " 'down_blocks.2.resnets.2.time_emb_proj.weight',\n",
       " 'down_blocks.2.resnets.2.time_emb_proj.bias',\n",
       " 'down_blocks.2.resnets.2.norm2.weight',\n",
       " 'down_blocks.2.resnets.2.norm2.bias',\n",
       " 'down_blocks.2.resnets.2.conv2.weight',\n",
       " 'down_blocks.2.resnets.2.conv2.bias',\n",
       " 'down_blocks.3.resnets.2.norm1.weight',\n",
       " 'down_blocks.3.resnets.2.norm1.bias',\n",
       " 'down_blocks.3.resnets.2.conv1.weight',\n",
       " 'down_blocks.3.resnets.2.conv1.bias',\n",
       " 'down_blocks.3.resnets.2.time_emb_proj.weight',\n",
       " 'down_blocks.3.resnets.2.time_emb_proj.bias',\n",
       " 'down_blocks.3.resnets.2.norm2.weight',\n",
       " 'down_blocks.3.resnets.2.norm2.bias',\n",
       " 'down_blocks.3.resnets.2.conv2.weight',\n",
       " 'down_blocks.3.resnets.2.conv2.bias',\n",
       " 'controlnet_down_blocks.4.weight',\n",
       " 'controlnet_down_blocks.4.bias',\n",
       " 'controlnet_down_blocks.7.weight',\n",
       " 'controlnet_down_blocks.7.bias',\n",
       " 'controlnet_down_blocks.8.weight',\n",
       " 'controlnet_down_blocks.8.bias',\n",
       " 'controlnet_down_blocks.12.weight',\n",
       " 'controlnet_down_blocks.12.bias',\n",
       " 'controlnet_down_blocks.13.weight',\n",
       " 'controlnet_down_blocks.13.bias',\n",
       " 'controlnet_down_blocks.14.weight',\n",
       " 'controlnet_down_blocks.14.bias',\n",
       " 'controlnet_down_blocks.15.weight',\n",
       " 'controlnet_down_blocks.15.bias',\n",
       " 'controlnet_proj_blocks.0.weight',\n",
       " 'controlnet_proj_blocks.0.bias',\n",
       " 'controlnet_proj_blocks.1.weight',\n",
       " 'controlnet_proj_blocks.1.bias',\n",
       " 'controlnet_proj_blocks.2.weight',\n",
       " 'controlnet_proj_blocks.2.bias',\n",
       " 'controlnet_proj_blocks.3.weight',\n",
       " 'controlnet_proj_blocks.3.bias',\n",
       " 'controlnet_proj_blocks.4.weight',\n",
       " 'controlnet_proj_blocks.4.bias',\n",
       " 'controlnet_proj_blocks.5.weight',\n",
       " 'controlnet_proj_blocks.5.bias',\n",
       " 'controlnet_proj_blocks.6.weight',\n",
       " 'controlnet_proj_blocks.6.bias',\n",
       " 'controlnet_proj_blocks.7.weight',\n",
       " 'controlnet_proj_blocks.7.bias',\n",
       " 'controlnet_proj_blocks.8.weight',\n",
       " 'controlnet_proj_blocks.8.bias',\n",
       " 'controlnet_proj_blocks.9.weight',\n",
       " 'controlnet_proj_blocks.9.bias',\n",
       " 'controlnet_proj_blocks.10.weight',\n",
       " 'controlnet_proj_blocks.10.bias',\n",
       " 'controlnet_proj_blocks.11.weight',\n",
       " 'controlnet_proj_blocks.11.bias',\n",
       " 'controlnet_proj_blocks.12.weight',\n",
       " 'controlnet_proj_blocks.12.bias',\n",
       " 'controlnet_proj_blocks.13.weight',\n",
       " 'controlnet_proj_blocks.13.bias',\n",
       " 'controlnet_proj_blocks.14.weight',\n",
       " 'controlnet_proj_blocks.14.bias',\n",
       " 'controlnet_proj_blocks.15.weight',\n",
       " 'controlnet_proj_blocks.15.bias',\n",
       " 'controlnet_mid_proj_block.weight',\n",
       " 'controlnet_mid_proj_block.bias']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_rnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
