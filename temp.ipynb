{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_movq_dict = dict(torch.load(\"/home/mlfavorfit/.cache/huggingface/hub/models--kandinsky-community--kandinsky-2-2-controlnet-depth/snapshots/4ecd717e8c9086cf4a16ca28b64894f70a42cd08/movq/diffusion_pytorch_model.bin\"))\n",
    "ct_unet_dict = dict(torch.load(\"/home/mlfavorfit/.cache/huggingface/hub/models--kandinsky-community--kandinsky-2-2-controlnet-depth/snapshots/4ecd717e8c9086cf4a16ca28b64894f70a42cd08/unet/diffusion_pytorch_model.bin\"))\n",
    "\n",
    "un_movq_dict = dict(load_file(\"/home/mlfavorfit/.cache/huggingface/hub/models--kandinsky-community--kandinsky-2-2-decoder-inpaint/snapshots/db790ad5cbcabed886f069ef2710774657621702/movq/diffusion_pytorch_model.safetensors\"))\n",
    "un_unet_dict = dict(load_file(\"/home/mlfavorfit/.cache/huggingface/hub/models--kandinsky-community--kandinsky-2-2-decoder-inpaint/snapshots/db790ad5cbcabed886f069ef2710774657621702/unet/diffusion_pytorch_model.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_movq = sorted([(cur, ct_movq_dict[cur].shape)for cur in ct_movq_dict], key=lambda x:x[0])\n",
    "ct_unet = sorted([(cur, ct_unet_dict[cur].shape)for cur in ct_unet_dict], key=lambda x:x[0])\n",
    "un_movq = sorted([(cur, un_movq_dict[cur].shape)for cur in un_movq_dict], key=lambda x:x[0])\n",
    "un_unet = sorted([(cur, un_unet_dict[cur].shape)for cur in un_unet_dict], key=lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ct, un in zip(ct_movq, un_movq):\n",
    "    if ct != un: print(\"warn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_ct = []\n",
    "common_ct_un = []\n",
    "for name, shape in ct_unet:\n",
    "    if name in un_unet_dict.keys():\n",
    "        common_ct_un.append(name)\n",
    "    else:\n",
    "        only_ct.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_in.weight\n"
     ]
    }
   ],
   "source": [
    "for name in common_ct_un:\n",
    "    if un_unet_dict[name].shape != ct_unet_dict[name].shape: print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 9, 3, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_unet_dict[\"conv_in.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 8, 3, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_unet_dict[\"conv_in.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_embedding.input_hint_block.0.bias torch.Size([16])\n",
      "add_embedding.input_hint_block.0.weight torch.Size([16, 3, 3, 3])\n",
      "add_embedding.input_hint_block.10.bias torch.Size([96])\n",
      "add_embedding.input_hint_block.10.weight torch.Size([96, 96, 3, 3])\n",
      "add_embedding.input_hint_block.12.bias torch.Size([256])\n",
      "add_embedding.input_hint_block.12.weight torch.Size([256, 96, 3, 3])\n",
      "add_embedding.input_hint_block.14.bias torch.Size([4])\n",
      "add_embedding.input_hint_block.14.weight torch.Size([4, 256, 3, 3])\n",
      "add_embedding.input_hint_block.2.bias torch.Size([16])\n",
      "add_embedding.input_hint_block.2.weight torch.Size([16, 16, 3, 3])\n",
      "add_embedding.input_hint_block.4.bias torch.Size([32])\n",
      "add_embedding.input_hint_block.4.weight torch.Size([32, 16, 3, 3])\n",
      "add_embedding.input_hint_block.6.bias torch.Size([32])\n",
      "add_embedding.input_hint_block.6.weight torch.Size([32, 32, 3, 3])\n",
      "add_embedding.input_hint_block.8.bias torch.Size([96])\n",
      "add_embedding.input_hint_block.8.weight torch.Size([96, 32, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for cur in only_ct:\n",
    "    print(cur, ct_unet_dict[cur].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384, 13, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 기존 텐서의 모양과 새로운 모양 정의\n",
    "old_shape = (384, 9, 3, 3)\n",
    "new_shape = (384, 13, 3, 3)\n",
    "\n",
    "# 새로운 텐서 생성 및 기존 텐서 값 복사\n",
    "new_weight = torch.zeros(new_shape)  # 새로운 텐서를 모두 0으로 초기화\n",
    "old_weight = un_unet_dict[\"conv_in.weight\"]\n",
    "new_weight[:, :old_shape[1], :, :] = old_weight\n",
    "\n",
    "# 추가된 부분에 랜덤한 0으로 채우기\n",
    "random_part = torch.zeros((new_shape[0], new_shape[1] - old_shape[1], new_shape[2], new_shape[3]))\n",
    "new_weight[:, old_shape[1]:, :, :] = random_part\n",
    "\n",
    "# 결과 확인\n",
    "print(new_weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_unet_dict[\"conv_in.weight\"] = new_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur in only_ct:\n",
    "    un_unet_dict[cur] = ct_unet_dict[cur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n",
      "torch.Size([384, 13, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(len(un_unet_dict))\n",
    "print(un_unet_dict[\"conv_in.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "save_file(un_unet_dict, \"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-inpainting-controlnet/unet/diffusion_pytorch_model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict = load_file(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-inpainting-controlnet/unet/diffusion_pytorch_model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n",
      "torch.Size([384, 13, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(len(temp_dict))\n",
    "print(temp_dict[\"conv_in.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def downgrade_image(img, bad_size=[64,64]):\n",
    "    img = img.resize(bad_size).resize(img.size)\n",
    "    return img\n",
    "\n",
    "from utils import make_canny_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "fns = glob(\"/media/mlfavorfit/sdb/contolnet_dataset/control_net_train_bad/images/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197967/197967 [46:24<00:00, 71.10it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_dir = \"/media/mlfavorfit/sdb/contolnet_dataset/control_net_train_bad/conditioning_images_canny\"\n",
    "for fn in tqdm(fns, total=len(fns)):\n",
    "    img = Image.open(fn)\n",
    "    bad_img = make_canny_condition(img)\n",
    "    bad_img.save(os.path.join(save_dir, os.path.basename(fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "temp_key = torch.load(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder/controlnet/ckpt_63000_0313/diffusion_pytorch_model.bin\", map_location=\"cpu\").keys()\n",
    "temp_key = list(temp_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "key_list = list(load_file(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder/unet/diffusion_pytorch_model.safetensors\").keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([cur for cur in key_list if not cur.startswith(\"up\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid_block.resnets.0.conv1.bias\n",
      "mid_block.resnets.0.conv1.weight\n",
      "mid_block.resnets.0.conv2.bias\n",
      "mid_block.resnets.0.conv2.weight\n",
      "mid_block.resnets.0.norm1.bias\n",
      "mid_block.resnets.0.norm1.weight\n",
      "mid_block.resnets.0.norm2.bias\n",
      "mid_block.resnets.0.norm2.weight\n",
      "mid_block.resnets.0.time_emb_proj.bias\n",
      "mid_block.resnets.0.time_emb_proj.weight\n",
      "mid_block.resnets.1.conv1.bias\n",
      "mid_block.resnets.1.conv1.weight\n",
      "mid_block.resnets.1.conv2.bias\n",
      "mid_block.resnets.1.conv2.weight\n",
      "mid_block.resnets.1.norm1.bias\n",
      "mid_block.resnets.1.norm1.weight\n",
      "mid_block.resnets.1.norm2.bias\n",
      "mid_block.resnets.1.norm2.weight\n",
      "mid_block.resnets.1.time_emb_proj.bias\n",
      "mid_block.resnets.1.time_emb_proj.weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 0\n",
    "for key in key_list:\n",
    "    if key in temp_key:\n",
    "        temp+=1\n",
    "        if \"mid\" in key:\n",
    "            print(key)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controlnet_cond_embedding.conv_in.weight\n",
      "controlnet_cond_embedding.conv_in.bias\n",
      "controlnet_cond_embedding.blocks.0.weight\n",
      "controlnet_cond_embedding.blocks.0.bias\n",
      "controlnet_cond_embedding.blocks.1.weight\n",
      "controlnet_cond_embedding.blocks.1.bias\n",
      "controlnet_cond_embedding.blocks.2.weight\n",
      "controlnet_cond_embedding.blocks.2.bias\n",
      "controlnet_cond_embedding.blocks.3.weight\n",
      "controlnet_cond_embedding.blocks.3.bias\n",
      "controlnet_cond_embedding.blocks.4.weight\n",
      "controlnet_cond_embedding.blocks.4.bias\n",
      "controlnet_cond_embedding.blocks.5.weight\n",
      "controlnet_cond_embedding.blocks.5.bias\n",
      "controlnet_cond_embedding.conv_out.weight\n",
      "controlnet_cond_embedding.conv_out.bias\n",
      "controlnet_down_blocks.0.weight\n",
      "controlnet_down_blocks.0.bias\n",
      "controlnet_down_blocks.1.weight\n",
      "controlnet_down_blocks.1.bias\n",
      "controlnet_down_blocks.2.weight\n",
      "controlnet_down_blocks.2.bias\n",
      "controlnet_down_blocks.3.weight\n",
      "controlnet_down_blocks.3.bias\n",
      "controlnet_down_blocks.4.weight\n",
      "controlnet_down_blocks.4.bias\n",
      "controlnet_down_blocks.5.weight\n",
      "controlnet_down_blocks.5.bias\n",
      "controlnet_down_blocks.6.weight\n",
      "controlnet_down_blocks.6.bias\n",
      "controlnet_down_blocks.7.weight\n",
      "controlnet_down_blocks.7.bias\n",
      "controlnet_down_blocks.8.weight\n",
      "controlnet_down_blocks.8.bias\n",
      "controlnet_down_blocks.9.weight\n",
      "controlnet_down_blocks.9.bias\n",
      "controlnet_down_blocks.10.weight\n",
      "controlnet_down_blocks.10.bias\n",
      "controlnet_down_blocks.11.weight\n",
      "controlnet_down_blocks.11.bias\n",
      "controlnet_down_blocks.12.weight\n",
      "controlnet_down_blocks.12.bias\n",
      "controlnet_down_blocks.13.weight\n",
      "controlnet_down_blocks.13.bias\n",
      "controlnet_down_blocks.14.weight\n",
      "controlnet_down_blocks.14.bias\n",
      "controlnet_down_blocks.15.weight\n",
      "controlnet_down_blocks.15.bias\n",
      "controlnet_mid_block.weight\n",
      "controlnet_mid_block.bias\n",
      "mid_block.attentions.0.norm.weight\n",
      "mid_block.attentions.0.norm.bias\n",
      "mid_block.attentions.0.proj_in.weight\n",
      "mid_block.attentions.0.proj_in.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "mid_block.attentions.0.proj_out.weight\n",
      "mid_block.attentions.0.proj_out.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 0\n",
    "for key in temp_key:\n",
    "    if key not in key_list:\n",
    "        print(key)\n",
    "        temp+=1\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 0\n",
    "for key in key_list:\n",
    "    if key not in temp_key:\n",
    "        temp+=1\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([cur for cur in key_list if not cur.startswith(\"up\")]) + 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlfavorfit/anaconda3/envs/diffusion_rnd/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "from PIL.ImageOps import expand\n",
    "\n",
    "from diffusers.utils import make_image_grid\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "from diffusers import KandinskyV22PriorPipeline\n",
    "from diffusers.pipelines.kandinsky2_2.pipline_kandinsky2_2_controlnet_split import KandinskyV22ControlnetPipeline\n",
    "\n",
    "from diffusers.models.controlnet_kandinsky import ControlNetModel as ControlNetModelKandinsky\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'center_input_sample': False} were passed to ControlNetModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot load <class 'diffusers.models.controlnet_kandinsky.ControlNetModel'> from /home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder because the following keys are missing: \n down_blocks.1.resnets.2.norm1.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm3.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight, controlnet_down_blocks.15.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight, down_blocks.0.resnets.2.norm1.bias, down_blocks.2.attentions.2.transformer_blocks.0.norm2.weight, down_blocks.1.resnets.2.time_emb_proj.weight, down_blocks.2.attentions.2.norm.weight, controlnet_down_blocks.15.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.0.attentions.2.transformer_blocks.0.norm3.bias, down_blocks.3.resnets.2.conv1.bias, down_blocks.0.resnets.2.norm1.weight, down_blocks.1.resnets.2.conv2.bias, down_blocks.0.resnets.2.norm2.bias, down_blocks.2.resnets.2.norm1.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm2.bias, down_blocks.0.resnets.2.conv2.weight, down_blocks.3.resnets.2.conv2.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, down_blocks.1.attentions.2.transformer_blocks.0.norm2.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight, controlnet_down_blocks.12.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight, down_blocks.0.resnets.2.time_emb_proj.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm1.bias, down_blocks.1.attentions.2.proj_out.weight, down_blocks.1.attentions.2.proj_in.bias, down_blocks.1.resnets.2.conv2.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight, down_blocks.2.attentions.2.proj_out.bias, down_blocks.0.attentions.2.proj_out.weight, down_blocks.0.resnets.2.conv1.weight, down_blocks.3.resnets.2.conv2.weight, down_blocks.0.attentions.2.proj_out.bias, down_blocks.0.attentions.2.transformer_blocks.0.norm3.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight, down_blocks.0.attentions.2.transformer_blocks.0.norm1.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, controlnet_proj_blocks.15.bias, down_blocks.3.resnets.2.norm2.weight, down_blocks.2.attentions.2.proj_in.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm1.weight, controlnet_proj_blocks.12.weight, down_blocks.1.resnets.2.time_emb_proj.bias, down_blocks.2.resnets.2.time_emb_proj.bias, down_blocks.2.attentions.2.proj_in.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, controlnet_proj_blocks.14.bias, down_blocks.0.attentions.2.norm.bias, controlnet_proj_blocks.13.bias, down_blocks.2.resnets.2.time_emb_proj.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm3.bias, controlnet_proj_blocks.12.bias, down_blocks.1.attentions.2.norm.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias, down_blocks.0.resnets.2.conv1.bias, down_blocks.2.resnets.2.conv2.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.0.attentions.2.proj_in.weight, down_blocks.2.attentions.2.proj_out.weight, controlnet_proj_blocks.13.weight, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias, down_blocks.1.resnets.2.norm1.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight, down_blocks.2.resnets.2.norm1.bias, down_blocks.1.attentions.2.transformer_blocks.0.norm2.bias, down_blocks.1.resnets.2.conv1.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight, down_blocks.0.attentions.2.transformer_blocks.0.norm2.weight, down_blocks.1.attentions.2.norm.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, down_blocks.2.resnets.2.conv1.bias, down_blocks.2.resnets.2.norm2.weight, down_blocks.1.attentions.2.proj_in.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm3.weight, down_blocks.1.resnets.2.norm2.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight, down_blocks.2.resnets.2.conv2.bias, down_blocks.3.resnets.2.norm1.bias, down_blocks.3.resnets.2.conv1.weight, down_blocks.0.resnets.2.conv2.bias, down_blocks.0.attentions.2.transformer_blocks.0.norm2.bias, down_blocks.2.attentions.2.transformer_blocks.0.norm1.bias, down_blocks.0.resnets.2.time_emb_proj.bias, down_blocks.2.resnets.2.conv1.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias, controlnet_down_blocks.12.weight, down_blocks.1.resnets.2.norm2.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight, down_blocks.1.resnets.2.conv1.weight, down_blocks.0.attentions.2.norm.weight, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm3.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight, down_blocks.3.resnets.2.norm1.weight, down_blocks.2.resnets.2.norm2.bias, down_blocks.3.resnets.2.norm2.bias, down_blocks.2.attentions.2.norm.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm1.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, down_blocks.3.resnets.2.time_emb_proj.weight, down_blocks.0.resnets.2.norm2.weight, controlnet_proj_blocks.15.weight, controlnet_down_blocks.14.bias, down_blocks.0.attentions.2.proj_in.bias, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias, controlnet_down_blocks.13.bias, controlnet_down_blocks.14.weight, down_blocks.1.attentions.2.proj_out.bias, down_blocks.3.resnets.2.time_emb_proj.bias, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight, controlnet_proj_blocks.14.weight, down_blocks.0.attentions.2.transformer_blocks.0.norm1.bias, controlnet_down_blocks.13.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight. \n Please make sure to pass `low_cpu_mem_usage=False` and `device_map=None` if you want to randomly initialize those weights or else make sure your checkpoint file is correct.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m decoder_model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m prior_model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-prior\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m controlnet_kandinsky \u001b[39m=\u001b[39m ControlNetModelKandinsky\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m     decoder_model_path, subfolder\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcontrolnet/tile_from_sd15\u001b[39;49m\u001b[39m\"\u001b[39;49m, torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16,\n\u001b[1;32m      6\u001b[0m )\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion_rnd/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/lib/favorfit/kjg/diffusers_research/diffusers_research/diffusers/models/modeling_utils.py:662\u001b[0m, in \u001b[0;36mModelMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m missing_keys \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(model\u001b[39m.\u001b[39mstate_dict()\u001b[39m.\u001b[39mkeys()) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(state_dict\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m    661\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_keys) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 662\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    663\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot load \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m because the following keys are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    664\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m missing: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(missing_keys)\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Please make sure to pass\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    665\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `low_cpu_mem_usage=False` and `device_map=None` if you want to randomly initialize\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m those weights or else make sure your checkpoint file is correct.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    667\u001b[0m     )\n\u001b[1;32m    669\u001b[0m unexpected_keys \u001b[39m=\u001b[39m load_model_dict_into_meta(\n\u001b[1;32m    670\u001b[0m     model,\n\u001b[1;32m    671\u001b[0m     state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     model_name_or_path\u001b[39m=\u001b[39mpretrained_model_name_or_path,\n\u001b[1;32m    675\u001b[0m )\n\u001b[1;32m    677\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_keys_to_ignore_on_load_unexpected \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load <class 'diffusers.models.controlnet_kandinsky.ControlNetModel'> from /home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder because the following keys are missing: \n down_blocks.1.resnets.2.norm1.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm3.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight, controlnet_down_blocks.15.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight, down_blocks.0.resnets.2.norm1.bias, down_blocks.2.attentions.2.transformer_blocks.0.norm2.weight, down_blocks.1.resnets.2.time_emb_proj.weight, down_blocks.2.attentions.2.norm.weight, controlnet_down_blocks.15.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.0.attentions.2.transformer_blocks.0.norm3.bias, down_blocks.3.resnets.2.conv1.bias, down_blocks.0.resnets.2.norm1.weight, down_blocks.1.resnets.2.conv2.bias, down_blocks.0.resnets.2.norm2.bias, down_blocks.2.resnets.2.norm1.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm2.bias, down_blocks.0.resnets.2.conv2.weight, down_blocks.3.resnets.2.conv2.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, down_blocks.1.attentions.2.transformer_blocks.0.norm2.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight, controlnet_down_blocks.12.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight, down_blocks.0.resnets.2.time_emb_proj.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm1.bias, down_blocks.1.attentions.2.proj_out.weight, down_blocks.1.attentions.2.proj_in.bias, down_blocks.1.resnets.2.conv2.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight, down_blocks.2.attentions.2.proj_out.bias, down_blocks.0.attentions.2.proj_out.weight, down_blocks.0.resnets.2.conv1.weight, down_blocks.3.resnets.2.conv2.weight, down_blocks.0.attentions.2.proj_out.bias, down_blocks.0.attentions.2.transformer_blocks.0.norm3.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight, down_blocks.0.attentions.2.transformer_blocks.0.norm1.weight, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, controlnet_proj_blocks.15.bias, down_blocks.3.resnets.2.norm2.weight, down_blocks.2.attentions.2.proj_in.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm1.weight, controlnet_proj_blocks.12.weight, down_blocks.1.resnets.2.time_emb_proj.bias, down_blocks.2.resnets.2.time_emb_proj.bias, down_blocks.2.attentions.2.proj_in.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, controlnet_proj_blocks.14.bias, down_blocks.0.attentions.2.norm.bias, controlnet_proj_blocks.13.bias, down_blocks.2.resnets.2.time_emb_proj.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm3.bias, controlnet_proj_blocks.12.bias, down_blocks.1.attentions.2.norm.weight, down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias, down_blocks.0.resnets.2.conv1.bias, down_blocks.2.resnets.2.conv2.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, down_blocks.0.attentions.2.proj_in.weight, down_blocks.2.attentions.2.proj_out.weight, controlnet_proj_blocks.13.weight, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias, down_blocks.1.resnets.2.norm1.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight, down_blocks.2.resnets.2.norm1.bias, down_blocks.1.attentions.2.transformer_blocks.0.norm2.bias, down_blocks.1.resnets.2.conv1.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight, down_blocks.0.attentions.2.transformer_blocks.0.norm2.weight, down_blocks.1.attentions.2.norm.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, down_blocks.2.resnets.2.conv1.bias, down_blocks.2.resnets.2.norm2.weight, down_blocks.1.attentions.2.proj_in.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm3.weight, down_blocks.1.resnets.2.norm2.bias, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight, down_blocks.2.resnets.2.conv2.bias, down_blocks.3.resnets.2.norm1.bias, down_blocks.3.resnets.2.conv1.weight, down_blocks.0.resnets.2.conv2.bias, down_blocks.0.attentions.2.transformer_blocks.0.norm2.bias, down_blocks.2.attentions.2.transformer_blocks.0.norm1.bias, down_blocks.0.resnets.2.time_emb_proj.bias, down_blocks.2.resnets.2.conv1.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias, controlnet_down_blocks.12.weight, down_blocks.1.resnets.2.norm2.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight, down_blocks.1.resnets.2.conv1.weight, down_blocks.0.attentions.2.norm.weight, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight, down_blocks.1.attentions.2.transformer_blocks.0.norm3.bias, down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight, down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight, down_blocks.3.resnets.2.norm1.weight, down_blocks.2.resnets.2.norm2.bias, down_blocks.3.resnets.2.norm2.bias, down_blocks.2.attentions.2.norm.bias, down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight, down_blocks.2.attentions.2.transformer_blocks.0.norm1.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, down_blocks.3.resnets.2.time_emb_proj.weight, down_blocks.0.resnets.2.norm2.weight, controlnet_proj_blocks.15.weight, controlnet_down_blocks.14.bias, down_blocks.0.attentions.2.proj_in.bias, down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias, controlnet_down_blocks.13.bias, controlnet_down_blocks.14.weight, down_blocks.1.attentions.2.proj_out.bias, down_blocks.3.resnets.2.time_emb_proj.bias, down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight, controlnet_proj_blocks.14.weight, down_blocks.0.attentions.2.transformer_blocks.0.norm1.bias, controlnet_down_blocks.13.weight, down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight. \n Please make sure to pass `low_cpu_mem_usage=False` and `device_map=None` if you want to randomly initialize those weights or else make sure your checkpoint file is correct."
     ]
    }
   ],
   "source": [
    "decoder_model_path = \"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder\"\n",
    "prior_model_path = \"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-prior\"\n",
    "\n",
    "controlnet_kandinsky = ControlNetModelKandinsky.from_pretrained(\n",
    "    decoder_model_path, subfolder=\"controlnet/tile_from_sd15\", torch_dtype=torch.float16,\n",
    ").eval()\n",
    "\n",
    "# prior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\n",
    "#     prior_model_path,\n",
    "#     use_safetensors=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "\n",
    "# pipeline = KandinskyV22ControlnetPipeline.from_pretrained(\n",
    "#     decoder_model_path,\n",
    "#     controlnet=controlnet_kandinsky,\n",
    "#     torch_dtype=torch.float16,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlfavorfit/lib/favorfit/kjg/diffusers_research/diffusers_research/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'diffusers.models.controlnet_kandinsky.ControlNetModel'>.load_config(...) followed by <class 'diffusers.models.controlnet_kandinsky.ControlNetModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "The config attributes {'center_input_sample': False} were passed to ControlNetModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "wei = ControlNetModelKandinsky.from_config(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder/controlnet/tile_from_sd15/config.json\").state_dict()\n",
    "w = torch.load(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder/controlnet/tile_from_sd15/diffusion_pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down_blocks.0.attentions.2.norm.weight\n",
      "down_blocks.0.attentions.2.norm.bias\n",
      "down_blocks.0.attentions.2.proj_in.weight\n",
      "down_blocks.0.attentions.2.proj_in.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.norm1.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.norm1.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.norm2.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.norm2.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.norm3.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.norm3.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight\n",
      "down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias\n",
      "down_blocks.0.attentions.2.proj_out.weight\n",
      "down_blocks.0.attentions.2.proj_out.bias\n",
      "down_blocks.0.resnets.2.norm1.weight\n",
      "down_blocks.0.resnets.2.norm1.bias\n",
      "down_blocks.0.resnets.2.conv1.weight\n",
      "down_blocks.0.resnets.2.conv1.bias\n",
      "down_blocks.0.resnets.2.time_emb_proj.weight\n",
      "down_blocks.0.resnets.2.time_emb_proj.bias\n",
      "down_blocks.0.resnets.2.norm2.weight\n",
      "down_blocks.0.resnets.2.norm2.bias\n",
      "down_blocks.0.resnets.2.conv2.weight\n",
      "down_blocks.0.resnets.2.conv2.bias\n",
      "down_blocks.1.attentions.2.norm.weight\n",
      "down_blocks.1.attentions.2.norm.bias\n",
      "down_blocks.1.attentions.2.proj_in.weight\n",
      "down_blocks.1.attentions.2.proj_in.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.norm1.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.norm1.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.norm2.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.norm2.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.norm3.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.norm3.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight\n",
      "down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias\n",
      "down_blocks.1.attentions.2.proj_out.weight\n",
      "down_blocks.1.attentions.2.proj_out.bias\n",
      "down_blocks.1.resnets.2.norm1.weight\n",
      "down_blocks.1.resnets.2.norm1.bias\n",
      "down_blocks.1.resnets.2.conv1.weight\n",
      "down_blocks.1.resnets.2.conv1.bias\n",
      "down_blocks.1.resnets.2.time_emb_proj.weight\n",
      "down_blocks.1.resnets.2.time_emb_proj.bias\n",
      "down_blocks.1.resnets.2.norm2.weight\n",
      "down_blocks.1.resnets.2.norm2.bias\n",
      "down_blocks.1.resnets.2.conv2.weight\n",
      "down_blocks.1.resnets.2.conv2.bias\n",
      "down_blocks.2.attentions.2.norm.weight\n",
      "down_blocks.2.attentions.2.norm.bias\n",
      "down_blocks.2.attentions.2.proj_in.weight\n",
      "down_blocks.2.attentions.2.proj_in.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.norm1.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.norm1.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.norm2.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.norm2.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.norm3.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.norm3.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight\n",
      "down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias\n",
      "down_blocks.2.attentions.2.proj_out.weight\n",
      "down_blocks.2.attentions.2.proj_out.bias\n",
      "down_blocks.2.resnets.2.norm1.weight\n",
      "down_blocks.2.resnets.2.norm1.bias\n",
      "down_blocks.2.resnets.2.conv1.weight\n",
      "down_blocks.2.resnets.2.conv1.bias\n",
      "down_blocks.2.resnets.2.time_emb_proj.weight\n",
      "down_blocks.2.resnets.2.time_emb_proj.bias\n",
      "down_blocks.2.resnets.2.norm2.weight\n",
      "down_blocks.2.resnets.2.norm2.bias\n",
      "down_blocks.2.resnets.2.conv2.weight\n",
      "down_blocks.2.resnets.2.conv2.bias\n",
      "down_blocks.3.resnets.2.norm1.weight\n",
      "down_blocks.3.resnets.2.norm1.bias\n",
      "down_blocks.3.resnets.2.conv1.weight\n",
      "down_blocks.3.resnets.2.conv1.bias\n",
      "down_blocks.3.resnets.2.time_emb_proj.weight\n",
      "down_blocks.3.resnets.2.time_emb_proj.bias\n",
      "down_blocks.3.resnets.2.norm2.weight\n",
      "down_blocks.3.resnets.2.norm2.bias\n",
      "down_blocks.3.resnets.2.conv2.weight\n",
      "down_blocks.3.resnets.2.conv2.bias\n",
      "controlnet_down_blocks.4.weight\n",
      "controlnet_down_blocks.4.bias\n",
      "controlnet_down_blocks.7.weight\n",
      "controlnet_down_blocks.7.bias\n",
      "controlnet_down_blocks.8.weight\n",
      "controlnet_down_blocks.8.bias\n",
      "controlnet_down_blocks.12.weight\n",
      "controlnet_down_blocks.12.bias\n",
      "controlnet_down_blocks.13.weight\n",
      "controlnet_down_blocks.13.bias\n",
      "controlnet_down_blocks.14.weight\n",
      "controlnet_down_blocks.14.bias\n",
      "controlnet_down_blocks.15.weight\n",
      "controlnet_down_blocks.15.bias\n",
      "controlnet_proj_blocks.0.weight\n",
      "controlnet_proj_blocks.0.bias\n",
      "controlnet_proj_blocks.1.weight\n",
      "controlnet_proj_blocks.1.bias\n",
      "controlnet_proj_blocks.2.weight\n",
      "controlnet_proj_blocks.2.bias\n",
      "controlnet_proj_blocks.3.weight\n",
      "controlnet_proj_blocks.3.bias\n",
      "controlnet_proj_blocks.4.weight\n",
      "controlnet_proj_blocks.4.bias\n",
      "controlnet_proj_blocks.5.weight\n",
      "controlnet_proj_blocks.5.bias\n",
      "controlnet_proj_blocks.6.weight\n",
      "controlnet_proj_blocks.6.bias\n",
      "controlnet_proj_blocks.7.weight\n",
      "controlnet_proj_blocks.7.bias\n",
      "controlnet_proj_blocks.8.weight\n",
      "controlnet_proj_blocks.8.bias\n",
      "controlnet_proj_blocks.9.weight\n",
      "controlnet_proj_blocks.9.bias\n",
      "controlnet_proj_blocks.10.weight\n",
      "controlnet_proj_blocks.10.bias\n",
      "controlnet_proj_blocks.11.weight\n",
      "controlnet_proj_blocks.11.bias\n",
      "controlnet_proj_blocks.12.weight\n",
      "controlnet_proj_blocks.12.bias\n",
      "controlnet_proj_blocks.13.weight\n",
      "controlnet_proj_blocks.13.bias\n",
      "controlnet_proj_blocks.14.weight\n",
      "controlnet_proj_blocks.14.bias\n",
      "controlnet_proj_blocks.15.weight\n",
      "controlnet_proj_blocks.15.bias\n",
      "controlnet_mid_proj_block.weight\n",
      "controlnet_mid_proj_block.bias\n"
     ]
    }
   ],
   "source": [
    "for cur in wei.keys():\n",
    "    if cur in w.keys():\n",
    "        if wei[cur].shape != w[cur].shape: \n",
    "            print(cur)\n",
    "            if cur.startswith(\"controlnet_down_blocks\") or cur.startswith(\"controlnet_proj\"):\n",
    "                wei[cur] = torch.zeros_like(wei[cur]) + 1e-6\n",
    "        else:\n",
    "            wei[cur] = w[cur] + 1e-6\n",
    "    else:\n",
    "        print(cur)\n",
    "        wei[cur] = torch.zeros_like(wei[cur])\n",
    "        if cur.startswith(\"controlnet_down_blocks\") or cur.startswith(\"controlnet_proj\"):\n",
    "            wei[cur] = torch.zeros_like(wei[cur]) + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(wei, \"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-decoder/controlnet/tile_from_sd15/diffusion_pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./temp.jsonl\", mode=\"r\") as f:\n",
    "    temp = [cur[:-1] for cur in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down_blocks.0.attentions.2.norm.weight',\n",
       " 'down_blocks.0.attentions.2.norm.bias',\n",
       " 'down_blocks.0.attentions.2.proj_in.weight',\n",
       " 'down_blocks.0.attentions.2.proj_in.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.norm1.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.norm1.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.norm2.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.norm2.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.norm3.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.norm3.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight',\n",
       " 'down_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias',\n",
       " 'down_blocks.0.attentions.2.proj_out.weight',\n",
       " 'down_blocks.0.attentions.2.proj_out.bias',\n",
       " 'down_blocks.0.resnets.2.norm1.weight',\n",
       " 'down_blocks.0.resnets.2.norm1.bias',\n",
       " 'down_blocks.0.resnets.2.conv1.weight',\n",
       " 'down_blocks.0.resnets.2.conv1.bias',\n",
       " 'down_blocks.0.resnets.2.time_emb_proj.weight',\n",
       " 'down_blocks.0.resnets.2.time_emb_proj.bias',\n",
       " 'down_blocks.0.resnets.2.norm2.weight',\n",
       " 'down_blocks.0.resnets.2.norm2.bias',\n",
       " 'down_blocks.0.resnets.2.conv2.weight',\n",
       " 'down_blocks.0.resnets.2.conv2.bias',\n",
       " 'down_blocks.1.attentions.2.norm.weight',\n",
       " 'down_blocks.1.attentions.2.norm.bias',\n",
       " 'down_blocks.1.attentions.2.proj_in.weight',\n",
       " 'down_blocks.1.attentions.2.proj_in.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.norm1.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.norm1.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.norm2.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.norm2.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.norm3.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.norm3.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight',\n",
       " 'down_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias',\n",
       " 'down_blocks.1.attentions.2.proj_out.weight',\n",
       " 'down_blocks.1.attentions.2.proj_out.bias',\n",
       " 'down_blocks.1.resnets.2.norm1.weight',\n",
       " 'down_blocks.1.resnets.2.norm1.bias',\n",
       " 'down_blocks.1.resnets.2.conv1.weight',\n",
       " 'down_blocks.1.resnets.2.conv1.bias',\n",
       " 'down_blocks.1.resnets.2.time_emb_proj.weight',\n",
       " 'down_blocks.1.resnets.2.time_emb_proj.bias',\n",
       " 'down_blocks.1.resnets.2.norm2.weight',\n",
       " 'down_blocks.1.resnets.2.norm2.bias',\n",
       " 'down_blocks.1.resnets.2.conv2.weight',\n",
       " 'down_blocks.1.resnets.2.conv2.bias',\n",
       " 'down_blocks.2.attentions.2.norm.weight',\n",
       " 'down_blocks.2.attentions.2.norm.bias',\n",
       " 'down_blocks.2.attentions.2.proj_in.weight',\n",
       " 'down_blocks.2.attentions.2.proj_in.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.norm1.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.norm1.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.norm2.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.norm2.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.norm3.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.norm3.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight',\n",
       " 'down_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias',\n",
       " 'down_blocks.2.attentions.2.proj_out.weight',\n",
       " 'down_blocks.2.attentions.2.proj_out.bias',\n",
       " 'down_blocks.2.resnets.2.norm1.weight',\n",
       " 'down_blocks.2.resnets.2.norm1.bias',\n",
       " 'down_blocks.2.resnets.2.conv1.weight',\n",
       " 'down_blocks.2.resnets.2.conv1.bias',\n",
       " 'down_blocks.2.resnets.2.time_emb_proj.weight',\n",
       " 'down_blocks.2.resnets.2.time_emb_proj.bias',\n",
       " 'down_blocks.2.resnets.2.norm2.weight',\n",
       " 'down_blocks.2.resnets.2.norm2.bias',\n",
       " 'down_blocks.2.resnets.2.conv2.weight',\n",
       " 'down_blocks.2.resnets.2.conv2.bias',\n",
       " 'down_blocks.3.resnets.2.norm1.weight',\n",
       " 'down_blocks.3.resnets.2.norm1.bias',\n",
       " 'down_blocks.3.resnets.2.conv1.weight',\n",
       " 'down_blocks.3.resnets.2.conv1.bias',\n",
       " 'down_blocks.3.resnets.2.time_emb_proj.weight',\n",
       " 'down_blocks.3.resnets.2.time_emb_proj.bias',\n",
       " 'down_blocks.3.resnets.2.norm2.weight',\n",
       " 'down_blocks.3.resnets.2.norm2.bias',\n",
       " 'down_blocks.3.resnets.2.conv2.weight',\n",
       " 'down_blocks.3.resnets.2.conv2.bias',\n",
       " 'controlnet_down_blocks.4.weight',\n",
       " 'controlnet_down_blocks.4.bias',\n",
       " 'controlnet_down_blocks.7.weight',\n",
       " 'controlnet_down_blocks.7.bias',\n",
       " 'controlnet_down_blocks.8.weight',\n",
       " 'controlnet_down_blocks.8.bias',\n",
       " 'controlnet_down_blocks.12.weight',\n",
       " 'controlnet_down_blocks.12.bias',\n",
       " 'controlnet_down_blocks.13.weight',\n",
       " 'controlnet_down_blocks.13.bias',\n",
       " 'controlnet_down_blocks.14.weight',\n",
       " 'controlnet_down_blocks.14.bias',\n",
       " 'controlnet_down_blocks.15.weight',\n",
       " 'controlnet_down_blocks.15.bias',\n",
       " 'controlnet_proj_blocks.0.weight',\n",
       " 'controlnet_proj_blocks.0.bias',\n",
       " 'controlnet_proj_blocks.1.weight',\n",
       " 'controlnet_proj_blocks.1.bias',\n",
       " 'controlnet_proj_blocks.2.weight',\n",
       " 'controlnet_proj_blocks.2.bias',\n",
       " 'controlnet_proj_blocks.3.weight',\n",
       " 'controlnet_proj_blocks.3.bias',\n",
       " 'controlnet_proj_blocks.4.weight',\n",
       " 'controlnet_proj_blocks.4.bias',\n",
       " 'controlnet_proj_blocks.5.weight',\n",
       " 'controlnet_proj_blocks.5.bias',\n",
       " 'controlnet_proj_blocks.6.weight',\n",
       " 'controlnet_proj_blocks.6.bias',\n",
       " 'controlnet_proj_blocks.7.weight',\n",
       " 'controlnet_proj_blocks.7.bias',\n",
       " 'controlnet_proj_blocks.8.weight',\n",
       " 'controlnet_proj_blocks.8.bias',\n",
       " 'controlnet_proj_blocks.9.weight',\n",
       " 'controlnet_proj_blocks.9.bias',\n",
       " 'controlnet_proj_blocks.10.weight',\n",
       " 'controlnet_proj_blocks.10.bias',\n",
       " 'controlnet_proj_blocks.11.weight',\n",
       " 'controlnet_proj_blocks.11.bias',\n",
       " 'controlnet_proj_blocks.12.weight',\n",
       " 'controlnet_proj_blocks.12.bias',\n",
       " 'controlnet_proj_blocks.13.weight',\n",
       " 'controlnet_proj_blocks.13.bias',\n",
       " 'controlnet_proj_blocks.14.weight',\n",
       " 'controlnet_proj_blocks.14.bias',\n",
       " 'controlnet_proj_blocks.15.weight',\n",
       " 'controlnet_proj_blocks.15.bias',\n",
       " 'controlnet_mid_proj_block.weight',\n",
       " 'controlnet_mid_proj_block.bias']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_rnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
