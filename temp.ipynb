{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_movq_dict = dict(torch.load(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/models--kandinsky-community--kandinsky-2-2-controlnet-depth/snapshots/4ecd717e8c9086cf4a16ca28b64894f70a42cd08/movq/diffusion_pytorch_model.bin\"))\n",
    "ct_unet_dict = dict(torch.load(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/models--kandinsky-community--kandinsky-2-2-controlnet-depth/snapshots/4ecd717e8c9086cf4a16ca28b64894f70a42cd08/unet/diffusion_pytorch_model.bin\"))\n",
    "\n",
    "un_movq_dict = dict(load_file(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/models--kandinsky-community--kandinsky-2-2-decoder-inpaint/snapshots/db790ad5cbcabed886f069ef2710774657621702/movq/diffusion_pytorch_model.safetensors\"))\n",
    "un_unet_dict = dict(load_file(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/models--kandinsky-community--kandinsky-2-2-decoder-inpaint/snapshots/db790ad5cbcabed886f069ef2710774657621702/unet/diffusion_pytorch_model.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_movq = sorted([(cur, ct_movq_dict[cur].shape)for cur in ct_movq_dict], key=lambda x:x[0])\n",
    "ct_unet = sorted([(cur, ct_unet_dict[cur].shape)for cur in ct_unet_dict], key=lambda x:x[0])\n",
    "un_movq = sorted([(cur, un_movq_dict[cur].shape)for cur in un_movq_dict], key=lambda x:x[0])\n",
    "un_unet = sorted([(cur, un_unet_dict[cur].shape)for cur in un_unet_dict], key=lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ct, un in zip(ct_movq, un_movq):\n",
    "    if ct != un: print(\"warn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_ct = []\n",
    "common_ct_un = []\n",
    "for name, shape in ct_unet:\n",
    "    if name in un_unet_dict.keys():\n",
    "        common_ct_un.append(name)\n",
    "    else:\n",
    "        only_ct.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_in.weight\n"
     ]
    }
   ],
   "source": [
    "for name in common_ct_un:\n",
    "    if un_unet_dict[name].shape != ct_unet_dict[name].shape: print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 9, 3, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_unet_dict[\"conv_in.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 8, 3, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_unet_dict[\"conv_in.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 9, 3, 3])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_unet_dict[\"conv_in.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_embedding.input_hint_block.0.bias torch.Size([16])\n",
      "add_embedding.input_hint_block.0.weight torch.Size([16, 3, 3, 3])\n",
      "add_embedding.input_hint_block.10.bias torch.Size([96])\n",
      "add_embedding.input_hint_block.10.weight torch.Size([96, 96, 3, 3])\n",
      "add_embedding.input_hint_block.12.bias torch.Size([256])\n",
      "add_embedding.input_hint_block.12.weight torch.Size([256, 96, 3, 3])\n",
      "add_embedding.input_hint_block.14.bias torch.Size([4])\n",
      "add_embedding.input_hint_block.14.weight torch.Size([4, 256, 3, 3])\n",
      "add_embedding.input_hint_block.2.bias torch.Size([16])\n",
      "add_embedding.input_hint_block.2.weight torch.Size([16, 16, 3, 3])\n",
      "add_embedding.input_hint_block.4.bias torch.Size([32])\n",
      "add_embedding.input_hint_block.4.weight torch.Size([32, 16, 3, 3])\n",
      "add_embedding.input_hint_block.6.bias torch.Size([32])\n",
      "add_embedding.input_hint_block.6.weight torch.Size([32, 32, 3, 3])\n",
      "add_embedding.input_hint_block.8.bias torch.Size([96])\n",
      "add_embedding.input_hint_block.8.weight torch.Size([96, 32, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for cur in only_ct:\n",
    "    print(cur, ct_unet_dict[cur].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384, 13, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 기존 텐서의 모양과 새로운 모양 정의\n",
    "old_shape = (384, 9, 3, 3)\n",
    "new_shape = (384, 13, 3, 3)\n",
    "\n",
    "# 새로운 텐서 생성 및 기존 텐서 값 복사\n",
    "new_weight = torch.zeros(new_shape)  # 새로운 텐서를 모두 0으로 초기화\n",
    "old_weight = un_unet_dict[\"conv_in.weight\"]\n",
    "new_weight[:, :old_shape[1], :, :] = old_weight\n",
    "\n",
    "# 추가된 부분에 랜덤한 값 채우기\n",
    "random_part = torch.rand((new_shape[0], new_shape[1] - old_shape[1], new_shape[2], new_shape[3]))\n",
    "new_weight[:, old_shape[1]:, :, :] = random_part\n",
    "\n",
    "# 결과 확인\n",
    "print(new_weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_unet_dict[\"conv_in.weight\"] = new_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur in only_ct:\n",
    "    un_unet_dict[cur] = ct_unet_dict[cur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n",
      "torch.Size([384, 13, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(len(un_unet_dict))\n",
    "print(un_unet_dict[\"conv_in.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "save_file(un_unet_dict, \"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-inpainting-controlnet/unet/diffusion_pytorch_model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict = load_file(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-inpainting-controlnet/unet/diffusion_pytorch_model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n",
      "torch.Size([384, 13, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(len(temp_dict))\n",
    "print(temp_dict[\"conv_in.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_embedding.input_hint_block.0.bias torch.Size([16])\n",
      "add_embedding.input_hint_block.0.weight torch.Size([16, 3, 3, 3])\n",
      "add_embedding.input_hint_block.10.bias torch.Size([96])\n",
      "add_embedding.input_hint_block.10.weight torch.Size([96, 96, 3, 3])\n",
      "add_embedding.input_hint_block.12.bias torch.Size([256])\n",
      "add_embedding.input_hint_block.12.weight torch.Size([256, 96, 3, 3])\n",
      "add_embedding.input_hint_block.14.bias torch.Size([4])\n",
      "add_embedding.input_hint_block.14.weight torch.Size([4, 256, 3, 3])\n",
      "add_embedding.input_hint_block.2.bias torch.Size([16])\n",
      "add_embedding.input_hint_block.2.weight torch.Size([16, 16, 3, 3])\n",
      "add_embedding.input_hint_block.4.bias torch.Size([32])\n",
      "add_embedding.input_hint_block.4.weight torch.Size([32, 16, 3, 3])\n",
      "add_embedding.input_hint_block.6.bias torch.Size([32])\n",
      "add_embedding.input_hint_block.6.weight torch.Size([32, 32, 3, 3])\n",
      "add_embedding.input_hint_block.8.bias torch.Size([96])\n",
      "add_embedding.input_hint_block.8.weight torch.Size([96, 32, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for cur in only_ct:\n",
    "    print(cur, temp_dict[cur].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-inpainting-controlnet\"\n",
    "        , subfolder=\"image_processor\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.4631865, 1.4631865, 1.4631865, ..., 1.9157377, 1.9157377,\n",
       "         1.9157377],\n",
       "        [1.4631865, 1.4631865, 1.4485881, ..., 1.9157377, 1.9157377,\n",
       "         1.9157377],\n",
       "        [1.4631865, 1.4631865, 1.4485881, ..., 1.9157377, 1.9157377,\n",
       "         1.9157377],\n",
       "        ...,\n",
       "        [1.5069818, 1.5069818, 1.5069818, ..., 1.8865409, 1.9011393,\n",
       "         1.9011393],\n",
       "        [1.5069818, 1.5069818, 1.5069818, ..., 1.9011393, 1.9157377,\n",
       "         1.9157377],\n",
       "        [1.5069818, 1.5069818, 1.5069818, ..., 1.9011393, 1.9157377,\n",
       "         1.9157377]],\n",
       "\n",
       "       [[1.1293944, 1.1293944, 1.1293944, ..., 1.4595653, 1.4595653,\n",
       "         1.4595653],\n",
       "        [1.1293944, 1.1293944, 1.1143867, ..., 1.4595653, 1.4595653,\n",
       "         1.4595653],\n",
       "        [1.1293944, 1.1293944, 1.1143867, ..., 1.4595653, 1.4595653,\n",
       "         1.4595653],\n",
       "        ...,\n",
       "        [1.1293944, 1.1293944, 1.1293944, ..., 1.219441 , 1.2344488,\n",
       "         1.2344488],\n",
       "        [1.1293944, 1.1293944, 1.1293944, ..., 1.2344488, 1.2494565,\n",
       "         1.2494565],\n",
       "        [1.1293944, 1.1293944, 1.1293944, ..., 1.2344488, 1.2494565,\n",
       "         1.2494565]],\n",
       "\n",
       "       [[1.705075 , 1.705075 , 1.705075 , ..., 1.3922336, 1.3922336,\n",
       "         1.3922336],\n",
       "        [1.705075 , 1.705075 , 1.690855 , ..., 1.3922336, 1.3922336,\n",
       "         1.3922336],\n",
       "        [1.705075 , 1.705075 , 1.690855 , ..., 1.3922336, 1.3922336,\n",
       "         1.3922336],\n",
       "        ...,\n",
       "        [1.7192951, 1.7192951, 1.7192951, ..., 1.3069133, 1.3211333,\n",
       "         1.3211333],\n",
       "        [1.7192951, 1.7192951, 1.7192951, ..., 1.3211333, 1.3353534,\n",
       "         1.3353534],\n",
       "        [1.7192951, 1.7192951, 1.7192951, ..., 1.3211333, 1.3353534,\n",
       "         1.3353534]]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image_processor(Image.open(\"/home/mlfavorfit/Desktop/style6.jpg\")).pixel_values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlfavorfit/anaconda3/envs/diffusion_rnd/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "from diffusers import DDPMScheduler, UNet2DConditionModel, VQModel, KandinskyV22PriorPipeline\n",
    "import torch\n",
    "\n",
    "movq = VQModel.from_pretrained(\n",
    "            \"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-inpainting-controlnet\", subfolder=\"movq\", torch_dtype=torch.float16\n",
    "        ).eval()\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    \"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-inpainting-controlnet\", subfolder=\"image_encoder\", torch_dtype=torch.float16\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(\"/home/mlfavorfit/Desktop/lib_link/favorfit/kjg/0_model_weights/diffusion/Kandinsky/kandinsky-inpainting-controlnet\", subfolder=\"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "movq.requires_grad_(False)\n",
    "movq.to(\"cuda\")\n",
    "image_encoder.requires_grad_(False)\n",
    "image_encoder.to(\"cuda\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unet.requires_grad_(True)\n",
    "unet.to(\"cuda\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in unet.parameters())\n",
    "total_params += sum(p.numel() for p in movq.parameters())\n",
    "total_params += sum(p.numel() for p in image_encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3166186251"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_rnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
